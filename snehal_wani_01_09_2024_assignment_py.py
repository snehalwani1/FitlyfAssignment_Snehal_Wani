# -*- coding: utf-8 -*-
"""Snehal_Wani_01-09-2024_Assignment.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P59UZfrwjSeuAbadnBrBkK5AZDGvNyvK
"""

from google.colab import drive
import pandas as pd
import numpy as np

path="/content/drive/MyDrive/Fitlyf/AssignmentData.xlsx"
funnel=pd.read_excel(path, sheet_name='WorkerFunnel')

funnel.head() # It displays first five rows of WorkerFunnel data

"""# Section 1 - Funnel Analysis

# **Q.1 Identify and appropriately handle the missing/blank and duplicate values in the dataset, and explain the logic behind your strategy in a short paragraph.**

**Exploratory Data Analysis**
"""

funnel.info()

funnel.dtypes

"""**1. Change Data Types**"""

funnel['Date']= pd.to_datetime(funnel['Date'])

funnel['Quarter']= funnel['Quarter'].astype('category')

funnel['Department']= funnel['Department'].astype('category')

funnel['Targeted Productivity']=pd.to_numeric(funnel['Targeted Productivity'].replace('What do you think should be here?',0), errors='coerce')

funnel['Overtime']= pd.to_numeric(funnel['Overtime'], errors='coerce')

funnel['No. of Workers']= pd.to_numeric(funnel['No. of Workers'], errors='coerce')
funnel['Actual Productivity']= pd.to_numeric(funnel['Actual Productivity'], errors='coerce')

"""**2. Missing Values and Duplicate handling**"""

missing_values = funnel.isnull().sum() # This code check for missing values
missing_values

"""There are 29 missing values in 'Actual Productivity' column"""

duplicate_values= funnel.duplicated().sum() # This code check for duplicate values
duplicate_values

"""**3. Handling missing values and duplicates**"""

funnel['Actual Productivity'].fillna(funnel['Actual Productivity'].mean(), inplace=True) # This code fill the missing values with mean of column

funnel.isnull().sum() # Check missing values are filled or not

funnel.drop_duplicates(inplace=True) # This code drop the duplicate value

funnel.duplicated().sum()

"""Now we can see there are no duplicate and missing values in dataset"""

funnel.describe() #Summary Statistics

"""# **Q. 2 Principal Component Analysis (PCA)**

**(i) Perform PCA on the following standardized features: Targeted Productivity, Overtime, No. of Workers, and Actual Productivity.**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

features=['Targeted Productivity', 'Overtime', 'No. of Workers', 'Actual Productivity'] # Selected features

#Standardized the features
scaler= StandardScaler()
funnel_scaled= scaler.fit_transform(funnel[features])

#Convert back to Dataframe for easier interpretation
funnel_scaled_df= pd.DataFrame(funnel_scaled, columns=features)

funnel_scaled_df.dropna(subset=features, inplace=True)

"""**Perform PCA**"""

#Apply PCA
pca=PCA()
pca_result = pca.fit_transform(funnel_scaled_df)

#Explained variance by each component
explained_variance= pca.explained_variance_ratio_
explained_variance

"""**(ii) Determine the number of principal components that explain at least 90% of the variance in the data.**"""

#Cumulative explained variance
cumulative_variance= np.cumsum(explained_variance)

#Number of components explaining 90% of variance
n_components= np.argmax(cumulative_variance >= 0.90)+1

print(n_components, cumulative_variance)

"""**(iii) Visualize the explained variance by each principal component.**"""

import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.plot(range(1,len(cumulative_variance)+1), cumulative_variance, marker='*', linestyle='--')
plt.xlabel('No. of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Expalined variance by Principal Components')
plt.grid(True)
plt.show()

"""Above code visualize how much variance explained as more principal components are added
- From the curve I can see that most of the data variance can be captired using the first two or three components, as the curve starts to flatten after the third component .
- This suggests the using two or three components is sufficient to capture majority of inforamtion in the dataset, making it an effective dimentionality reduction technique

# **Q. 3 Predictive Modeling and Time Series Analysis**

**(i) Build an ARIMA model to forecast the Actual Productivity for the next four quarters (four weeks).**
"""

from statsmodels.tsa.arima.model import ARIMA

#Extract Actual Productivity Series
productivity_series= funnel['Actual Productivity']

# Fit the data into ARIMA model
model = ARIMA(productivity_series, order=(1,1,1))
model_fit= model.fit()

# forecast next 4 weeks
forecast= model_fit.forecast(steps=4)

forecast

"""**(ii) Evaluate the model using Mean Absolute Percentage Error (MAPE) and Mean Squared Error (MSE).**"""

from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

#Calculate MAPE and MSE
mape=mean_absolute_percentage_error(productivity_series[-4:],forecast)
mse= mean_squared_error(productivity_series[-4:], forecast)

print("mean_absolute_percentage_error= ", mape)
print("mean_squared_error= ", mse)

"""**(iii) Visualize the forecasted vs actual productivity values, and interpret the modelâ€™s accuracy.**"""

plt.figure(figsize=(10,6))
plt.plot(productivity_series.index[-10:], productivity_series[-10:], label='Actual Productivity')
plt.plot(forecast.index, forecast, label='Forcasted Productivity', linestyle='--' )
plt.xlabel('Date')
plt.ylabel('Productivity')
plt.title('Actal vs Forcasted Productivity')
plt.legend()
plt.show()

"""# Q.4 **Culstering Analysis**

**(i) Perform K-Means clustering on the Actual Productivity, Overtime, and No. of Workers.**
"""

from sklearn.cluster import KMeans
# Select relevant features for clustering
cluster_features= funnel[['Actual Productivity','Overtime', 'No. of Workers']]

#cluster_features.dropna(inplace=True)

print(len(funnel))
print(len(cluster_features))

cluster_features= cluster_features.fillna(cluster_features.mean())

#funnel= pd.DataFrame()

#Apply KMeans Clusteing
kmeans=KMeans(n_clusters=3, random_state=42)
funnel['Cluster']= kmeans.fit_predict(cluster_features)

#Cluster centriods
kmeans.cluster_centers_

"""**(ii) Determine the optimal number of clusters using the Elbow method.**"""

sse=[]
for k in range(1,10):
  kmeans=KMeans(n_clusters=k, random_state=42)
  kmeans.fit(cluster_features)
  sse.append(kmeans.inertia_)

#Plot SSE against number of clusters

plt.figure(figsize=(8,6))
plt.plot(range(1,10), sse, marker='*', linestyle='--')
plt.xlabel('Number of Clusters')
plt.ylabel('SSE')
plt.title('Clbow MEthod for Optimal Number of CLusters')
plt.grid(True)
plt.show()

"""
**(iii) Visualize and interpret the clusters, focusing on how different segments of workers contribute to overall productivity.**"""

#Visualize the clusters
import seaborn as sns

plt.figure(figsize=(10,6))
sns.scatterplot(x='Overtime', y='Actual Productivity', hue='Cluster', data=funnel, palette='Set1')
plt.xlabel('Overtime')
plt.ylabel('Actual Productivity')
plt.title('KMeans Clustering of workers')
plt.show()

"""#**Section 2: Anomaly Detection**

**Q.1 Data Import and Exploration**
"""

path="/content/drive/MyDrive/Fitlyf/AssignmentData.xlsx"
transaction=pd.read_excel(path, sheet_name='creditcard')

"""**Explaoratory Data Analysis**"""

transaction.head() #To see first five rows

transaction.isnull().sum()

transaction.dtypes

transaction['V2']= pd.to_numeric(transaction['V2'].replace("I think you're doing good!",0), errors='coerce')

transaction['V7'] = pd.to_numeric(transaction['V7'], errors='coerce')

transaction['V9'] = pd.to_numeric(transaction['V9'], errors='coerce')

transaction['V24'] = pd.to_numeric(transaction['V24'], errors='coerce')

transaction.isna().sum()

transaction.dropna(subset=['V7', 'V9', 'V24'], inplace=True)

transaction.dtypes

print(transaction['Class'].value_counts()) #To see ratio of fradulant to non fradulant

"""**Visualize the distribution of transaction amounts for both fraudulent and non-fraudulent transactions.**

"""

plt.figure(figsize=(12,6))

#Plot for Normal Transactions
sns.histplot(transaction[transaction['Class']==0]['Amount'], bins=50, kde=True, color='blue',label='Normal')



#Plot for Fradulant Transactions
sns.histplot(transaction[transaction['Class']==1]['Amount'], bins=50, kde=True, color='red',label='Fradulant')

plt.legend()
plt.title('Transaction Amount Distribution')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

"""# Q.2 **Feature Engineering**

**scaling on the Amount and Time features**
"""

from sklearn.preprocessing import MinMaxScaler
import joblib

scaler= MinMaxScaler()
transaction[['Amount', 'Time']]= scaler.fit_transform(transaction[['Amount', 'Time']])

joblib.dump(scaler, 'scaler.pkl')

"""**Apply PCA for visualization**"""

features= transaction.drop('Class',axis=1)
pca= PCA(n_components=2)
pca_features= pca.fit_transform(features)

#Add PCA features back to dataframe
pca_df= pd.DataFrame(pca_features, columns=['PC1','PC2'])
pca_df['Class']= transaction['Class'].values

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1',y='PC2', hue='Class', data=pca_df, palette='dark', alpha=0.5)
plt.title('PCA of Transactions')
plt.show()

import joblib
joblib.dump(features.columns.tolist(), 'featurename.pkl')

from google.colab import files
files.download('/content/featurename.pkl')

"""# Q.3 **Anomaly Detection Model**

**Isolation Forest**
"""

from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, roc_auc_score

#Train Isolation Forest
iso_forest= IsolationForest(contamination=0.001, random_state=42)
iso_forest.fit_predict(features)

import joblib
joblib.dump(iso_forest, 'iso_forest_model1.pkl')

transaction['anomaly']= iso_forest.fit_predict(features)

#Convert predictions to binary
transaction['anomaly']=transaction['anomaly'].map({1:0, -1:1})

#Evaluate
print(classification_report(transaction['Class'], transaction['anomaly']))
print('ROC-AUC: ', roc_auc_score(transaction['Class'], transaction['anomaly']))



mmmmmmmmmmm#import joblib
#joblib.dump(iso_forest, 'iso_forest_model.pkl')

#from sklearn.ensemble import IsolationForest
#from sklearn.metrics import classification_report, roc_auc_score

#Train Isolation Forest
#iso_forest= IsolationForest(contamination=0.001, random_state=42)
#transaction['anomaly']= iso_forest.fit_predict(features)

#Convert predictions to binary
#transaction['anomaly']=transaction['anomaly'].map({1:0, -1:1})

#Evaluate
#print(classification_report(transaction['Class'], transaction['anomaly']))
#print('ROC-AUC: ', roc_auc_score(transaction['Class'], transaction['anomaly']))mmmmmmmmmmmmmmmmmmmmmmmmmmmmm

#

"""**Local Outlier Factor(LOF)**"""

from sklearn.neighbors import LocalOutlierFactor

# Train LOF
lof= LocalOutlierFactor(n_neighbors=20,contamination=0.001)
transaction['anomaly']= lof.fit_predict(features)



#Convert predictions to binary
transaction['anomaly']=transaction['anomaly'].map({1:0, -1:1})

pca_df['anomaly']=transaction['anomaly'].reset_index(drop=True) #Add anomaly prediction to pca_df

pca_df.columns

# Evaluate
print(classification_report(transaction['Class'], transaction['anomaly']))
print('ROC-AUC: ', roc_auc_score(transaction['Class'], transaction['anomaly']))

"""# Q.4 **Visualizing Anomalies**"""

pca_df.columns

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1',y='PC2', hue='anomaly', data=pca_df, palette='coolwarm', alpha=0.5)
plt.title('PCA Features with Anomalies')
plt.show()

from google.colab import files
files.download('/content/iso_forest_model1.pkl')
files.download('/content/scaler.pkl')

"""**Function for new Dataset**"""

def detect_fradulent_transactions(new_data, model):

  #Apply scaling
  new_data[['Amount', 'Time']]= scaler.transform(new_data[['Amount','Time']])

  #Predict anomalies
  predictions= model.predict(new_data.drop('Class',axis=1))
  predictions= pd.Series(predictions),map({1:0, -1:1})

  return new_data[predictions==1]

"""**Develping a streamlit app**

"""

pip install streamlit

import streamlit as st

#path="/content/drive/MyDrive/Fitlyf/AssignmentData.xlsx"
#transaction=pd.read_excel(path, sheet_name='creditcard')

#function to detect fradulant transactions
def detect_fraudulent_transactions(new_data, iso_forest, scaler):

  #Apply scaling
  new_data[['Amount', 'Time']]= scaler.transform(new_data[['Amount','Time']])

  #Predict anomalies
  predictions= model.predict(new_data.drop('Class',axis=1))
  predictions= pd.Series(predictions),map({1:0, -1:1}) #1 for Normal, -1 for anomaly

  return new_data[predictions==1]

#Load pre-trained model and scaler
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings("ignore", category=UserWarning, module='streamlit')

iso_forest= IsolationForest(contamination=0.001, random_state=42)
scaler= MinMaxScaler()

#Streamlit app
st.title("Credit Card Fraud Detection")
st.write("Upload a excel file with credit card transactions to detect frauds")

#path="/content/drive/MyDrive/Fitlyf/AssignmentData.xlsx"
uploaded_file= st.file_uploader ("Choose an Excel File", type=['xlsx','xls'])
if uploaded_file is not None:
  try:
    #  Read excel file
    new_transactions= pd.read_excel(uploaded_file, sheet_name='creditcard_test')

    #Check if necessary columns are present in uploaded file
    required_columns= ['Amount','Time','Class']
    if all (column in new_transactions.columns for column in required_columns):
      #Scale data
      new_transactions[['Amount','Time']]= scaler.fit_transform(new_transactions[['Amount','Time']])

      #Detect Fradulat transactions
      fradulant_transactions= detect_fraudulent_transactions(new_transactions, iso_forest, scaler)

      #Dispaly results
      st.write(f"Number of fradulant transactions detected: {len(fradulant_transactions)}")
      st.dataframe(fradulant_transactions)

      #Visualization
      if not fradulant_transactions.empty:
        plt.figure(figsize=(10,6))
        sns.scatterplot(x='Amount', y='Time', hue=fradulant_transactions['Class'], data=fradulant_transactions)
        plt.title('Detected Fradulant Transactions')
        st.pyplot(plt)
    else:
      st.write("The uploaded file does not contain the required columns: 'Amount', 'Time', 'Class' ")
  except Exception as e:
      st.write(f"An error occured while reading the Excel file: {e}")
else:
  st.write("Please upload a file to proceed")

